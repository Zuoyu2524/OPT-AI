{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed optimization in Pytorch\n",
        "Alexandre Reiffers-Masson\n",
        "\n",
        "Nom(s) et prénom(s): ZHANG Zuoyu"
      ],
      "metadata": {
        "id": "bZ-Ma-XCukMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this laboratory, we will focus on programming a distributed version of the estimator proposed in \"Sensor Selection via Convex Optimization\" written by Siddharth Joshi and Stephen Boyd. The context is the following: \n",
        "We want to estimate a vector $x\\in\\mathbb{R}^n$ from $m$ linear measurements, corrupted by additive noise:\n",
        "$$\n",
        "y_i = a_i^T x+v_i,\n",
        "$$\n",
        "where $v_i$ is an independent identically distributed $\\mathcal{N}(0,\\sigma^2)$. $y_i$ and $a_i$ is known by sensor $i$. To estimate $x$, we will solve in a distributed way the following optimization problem:\n",
        "$$\n",
        "\\min_x \\sum_{i=1}^m(y_i-a_i^T x)^2.\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "DYSh4VYK0pGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1/ Compute the argmin of the previous optimization problem. \n"
      ],
      "metadata": {
        "id": "KlxkEAOJ9OcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To $\\min_x \\sum_{i=1}^m(y_i-a_i^T x)^2$, we firstly let $f(x) = \\min_x \\sum_{i=1}^m(y_i-a_i^T x)^2$, calculate the derivative of $f(x)$ and to find the minimum, we let the derivative be zero\n",
        "$$\\frac{\\partial f}{\\partial x} = 2\\sum_{i=1}^m(-a_i)(y_i-a_i^T x) = 0$$\n",
        "we got\n",
        "$$\\sum_{i=1}^m a_i a_i^T x = \\sum_{i=1}^m a_i y_i$$\n",
        "Further, we can get the minimum $x$ of the optimization problem\n",
        "$$\\hat{x} = (\\sum_{i=1}^m a_i a_i^T)^{-1} \\sum_{i=1}^m a_i y_i$$"
      ],
      "metadata": {
        "id": "7jXVP8oHJ6K4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2/ Can you describe a simple iterative algorithms to solve this problem?\n",
        "\n"
      ],
      "metadata": {
        "id": "0f5_-wAL9Pq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think we can use the gradient descent algorithm to optimize this problem. To find the local minimum of a function using gradient descent, one must iteratively search for points at a specified step distance in the opposite direction of the corresponding gradient (or approximate gradient) at the current point on the function. Proceed as follows:\n",
        "\n",
        "1.   Initialize $x_0$, step size $γ$\n",
        "2.   for $k = 1$ to Max iter:\n",
        "3.   >$$\\nabla f(x^{(k-1)}) = 2\\sum_{i=1}^m(-a_i)(y_i-a_i^T x^{(k-1)})$$  \n",
        "4.   >$$x^{(k)} = x^{(k-1)} - γ \\nabla f(x^{(k-1)})$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vfl1Xd6eP8dK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/ Assuming that node $i$ can broadcast vectors to the other nodes, what could be a possible information it should broadcast to allow every node to compute the optimal $x$? \n",
        "\n",
        "*Constraint:* Remember that you would to avoid that a node is inversing a matrix and you want to minimize the number of operations made by each node\n",
        "\n",
        "              Call the professor when you have a satisfying answer. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J6r6UQ3X9Qw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve the optimization problem in a distributed way, each node should broadcast information that allows every other node to update their estimate of the vector $x$ based on their local measurements and the information received from other nodes.\n",
        "\n",
        "One way to achieve this is through a consensus-based algorithm. Each node can maintain a local estimate $x_i$ of the vector $x$ , and broadcast this estimate to its neighbors. Then, each node updates its estimate based on the estimates received from its neighbors, using a weighted average:\n",
        "\n",
        "$$x_i(t+1) = (1 - \\alpha)x_i(t) + \\frac{\\alpha}{ki} \\sum_{(j \\, in \\, N(i))}(x_j(t))$$\n",
        "\n",
        "where $N(i)$ is the neighbors of node $i$, $k_i$ is the number of neighbors of node $i$ , $α$ is a weight that controls the rate of convergence of the algorithm, and neighbors $i$ denotes the set of neighbors of node $i$ .\n",
        "\n",
        "To incorporate the information from the measurements, each node can also maintain a local residual $r_i$ defined as:\n",
        "\n",
        "$$r_i(t) = y_i - a_i T(x_i(t))$$\n",
        "\n",
        "Then, each node can use the residuals received from its neighbors to update its local estimate, using a weighted least squares formulation:\n",
        "\n",
        "$$x_i(t+1) = argmin_x \\sum_{(j \\, in \\, N(i))} (w_j(t)(y_j - a_jT(x)))^2$$\n",
        "\n",
        "where $w_j(t)$ is a weight that captures the confidence in the residual received from node $j$ , and can be computed based on the noise variance $\\sigma^2$ and the number of iterations of the algorithm.\n",
        "\n",
        "By iteratively updating their estimates based on the local measurements and the information received from neighbors, the nodes can converge to a consensus estimate of the vector $x$ . Once the consensus is reached, each node can compute the optimal estimate of $x$ using its local measurements and the consensus estimate. Specifically, each node can compute:\n",
        "\n",
        "$$x_{opt} = argmin_x \\sum_{i=1}^{m} (y_i - a_i^T x)^2$$\n",
        "\n",
        "using its local measurements and the consensus estimate of $x$.\n",
        "\n",
        "Overall, the information that each node needs to broadcast includes its local estimate $x_i$ , its local residual $r_i$ , and the weights $w_j(t)$ for each neighbor $j$ . These weights can be computed based on the noise variance $\\sigma^2$ and the number of iterations of the algorithm. By broadcasting this information to its neighbors, each node can update its estimate of $x$ in a distributed way, and converge to a consensus estimate of $x$."
      ],
      "metadata": {
        "id": "10shmXWPJgCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will move now to the implementation of the distributed algorithm in pytroch. Please first have a look to the following two tutorials to learn more about distribution in pytroch:\n",
        "- https://pytorch.org/tutorials/beginner/dist_overview.html\n",
        "- https://pytorch.org/tutorials/intermediate/dist_tuto.html"
      ],
      "metadata": {
        "id": "xvcjoxuh_fXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excute this cell to install the required packages for the lab. "
      ],
      "metadata": {
        "id": "SHG-5BHaAMVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yd-yDCkZZQkI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import torch\n",
        "import math\n",
        "from torch import distributions\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "from random import choices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4/ For the implementation of your distributed algorithm, what can of *Collective Communication* will you use? Complete and comment the code below to test the chosen Communication protocol. "
      ],
      "metadata": {
        "id": "CkTzOklFBEhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(rank, size):\n",
        "    group = dist.new_group(list(range(size)))\n",
        "    tensor = torch.randn(2,2)\n",
        "    print('Rank ', rank, ' has data ', tensor, \"before communication\")\n",
        "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group) # perform all_reduce operation\n",
        "    print('Rank ', rank, ' has data ', tensor, \"after communication\")\n",
        "\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\n",
        "size = 4\n",
        "processes = []\n",
        "\n",
        "for rank in range(size):\n",
        "  p = mp.Process(target=init_process, args=(rank, size,run))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "id": "RF26hpOn_WPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f5599a-4fc5-4da7-810b-8900bf2fc034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank  Rank Rank Rank   1 2 03 has data      has data  has data  has data  tensor([[-0.0091, -1.4265],\n",
            "        [ 1.2028,  1.8545]])  before communicationtensor([[-0.0091, -1.4265],\n",
            "        [ 1.2028,  1.8545]])tensor([[-0.0091, -1.4265],\n",
            "        [ 1.2028,  1.8545]])\n",
            "   before communication\n",
            "tensor([[-0.0091, -1.4265],\n",
            "        [ 1.2028,  1.8545]]) before communicationbefore communication\n",
            "\n",
            "Rank Rank Rank Rank     0132     has data  has data  has data  has data     tensor([[-0.0366, -5.7062],\n",
            "        [ 4.8111,  7.4180]])tensor([[-0.0366, -5.7062],\n",
            "        [ 4.8111,  7.4180]])tensor([[-0.0366, -5.7062],\n",
            "        [ 4.8111,  7.4180]])  tensor([[-0.0366, -5.7062],\n",
            "        [ 4.8111,  7.4180]]) after communicationafter communication after communication\n",
            "\n",
            "after communication\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "              Call the professor when you have a satisfying answer. \n"
      ],
      "metadata": {
        "id": "91MJUObHPR13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5/ Create the dataset with the matrix $A = [[5., 1.0], [1.0, 4.]]$, $X = [[8.], [2.]]$ and $\\sigma =0.5$.  "
      ],
      "metadata": {
        "id": "IulQUh9sPhcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigma = 0.5\n",
        "dtype = torch.float\n",
        "nb_samples = 2\n",
        "A = np.array([[5., 1.0], [1.0, 4.]]).astype(np.float32)\n",
        "A_pytorch = torch.tensor(A)\n",
        "X = np.array([[8.], [2.]]).astype(np.float32)\n",
        "X_pytorch = torch.tensor(X)\n",
        "y = torch.mm(A_pytorch,X_pytorch) + torch.normal(mean=0,std=sigma,size=[2,1]) "
      ],
      "metadata": {
        "id": "IWmwzMWKatjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6/ Complete the following code to implement your algorithm. \n",
        "\n",
        "              Call the professor when you have a satisfying answer. \n"
      ],
      "metadata": {
        "id": "NPHMvps9QaCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the gradient descent algorithm for this optimization problem, we need to calculate the gradient of the objective function with respect to x. The objective function is defined as:\n",
        "\n",
        "$$L(x) = ∑_{i=1}^m(y_i−a^T_ix)^2$$\n",
        "\n",
        "where $y_i = a^T_ix + v_i$, and $v_i$ is a random noise term.\n",
        "\n",
        "The gradient of $L(x)$ with respect to $x$ can be calculated as:\n",
        "\n",
        "$$∇L(x) = -2A(y - Ax)$$\n",
        "\n",
        "where $A$ is the matrix we have created before, $y$ is the vector of observations, and $x$ is the vector of unknowns.\n",
        "\n",
        "Using this gradient, we can update $x$ as:\n",
        "\n",
        "$x ← x - \\gamma ∇L(x)$\n",
        "\n",
        "where $\\gamma$ is the step size.\n",
        "\n",
        "Based on this, we can implement the gradient descent algorithm as follows:"
      ],
      "metadata": {
        "id": "EVFx7-7h1wAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step_size = 0.01\n",
        "nb_epochs_1 = 30\n",
        "\n",
        "def objective_func(x,y,A):\n",
        "    return torch.sum((y - torch.matmul(A, x)) ** 2)\n",
        "\n",
        "def gradient(x,y,A):\n",
        "    return -2 * torch.matmul(A.t(), (y - torch.matmul(A, x)))\n",
        "\n",
        "def run(rank, size):\n",
        "    # Initialize x with random values\n",
        "    x = torch.randn(A_pytorch.shape[1], 1)\n",
        "\n",
        "    for epoch in range(nb_epochs_1):\n",
        "        # Calculate gradient and update x\n",
        "        grad = gradient(x,y,A_pytorch)\n",
        "        x = x - step_size * grad\n",
        "\n",
        "        # Average x across processes\n",
        "        dist.all_reduce(x, op=dist.ReduceOp.SUM)\n",
        "        x /= size\n",
        "\n",
        "        # Print progress\n",
        "        if rank == 0:\n",
        "            print(f\"Epoch {epoch+1}: x = {x.flatten()}\")\n",
        "\n",
        "    # Calculate final objective value\n",
        "    dist.barrier()\n",
        "    obj_val = objective_func(x,y,A_pytorch)\n",
        "    dist.all_reduce(obj_val, op=dist.ReduceOp.SUM)\n",
        "    obj_val /= size\n",
        "\n",
        "    # Print final result\n",
        "    if rank == 0:\n",
        "        print(f\"Final result: x = {x.flatten()}, L(x) = {obj_val.item()}\")\n",
        "\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\n",
        "size = 2\n",
        "processes = []\n",
        "\n",
        "for rank in range(size):\n",
        "  p = mp.Process(target=init_process, args=(rank, size,run))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "id": "iQ7ZLg4fa9t-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ace4cb-6aaf-4b89-a636-1addcb34c099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: x = tensor([4.7488, 3.0703])\n",
            "Epoch 2: x = tensor([6.2320, 3.2345])\n",
            "Epoch 3: x = tensor([6.9145, 3.0758])\n",
            "Epoch 4: x = tensor([7.2706, 2.8483])\n",
            "Epoch 5: x = tensor([7.4825, 2.6340])\n",
            "Epoch 6: x = tensor([7.6227, 2.4545])\n",
            "Epoch 7: x = tensor([7.7224, 2.3107])\n",
            "Epoch 8: x = tensor([7.7961, 2.1979])\n",
            "Epoch 9: x = tensor([7.8518, 2.1101])\n",
            "Epoch 10: x = tensor([7.8943, 2.0422])\n",
            "Epoch 11: x = tensor([7.9269, 1.9897])\n",
            "Epoch 12: x = tensor([7.9521, 1.9492])\n",
            "Epoch 13: x = tensor([7.9714, 1.9180])\n",
            "Epoch 14: x = tensor([7.9863, 1.8939])\n",
            "Epoch 15: x = tensor([7.9978, 1.8753])\n",
            "Epoch 16: x = tensor([8.0067, 1.8609])\n",
            "Epoch 17: x = tensor([8.0135, 1.8498])\n",
            "Epoch 18: x = tensor([8.0188, 1.8413])\n",
            "Epoch 19: x = tensor([8.0229, 1.8347])\n",
            "Epoch 20: x = tensor([8.0260, 1.8297])\n",
            "Epoch 21: x = tensor([8.0284, 1.8257])\n",
            "Epoch 22: x = tensor([8.0303, 1.8227])\n",
            "Epoch 23: x = tensor([8.0317, 1.8204])\n",
            "Epoch 24: x = tensor([8.0328, 1.8186])\n",
            "Epoch 25: x = tensor([8.0337, 1.8172])\n",
            "Epoch 26: x = tensor([8.0344, 1.8161])\n",
            "Epoch 27: x = tensor([8.0349, 1.8153])\n",
            "Epoch 28: x = tensor([8.0353, 1.8147])\n",
            "Epoch 29: x = tensor([8.0356, 1.8142])\n",
            "Epoch 30: x = tensor([8.0358, 1.8138])\n",
            "Final result: x = tensor([8.0358, 1.8138]), L(x) = 2.5671091862022877e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7/ Suggest and implement an algorithm based on distributed consensus optimization.  \n",
        " "
      ],
      "metadata": {
        "id": "Y71qHmvMRpYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed consensus optimization is a technique used in distributed machine learning to achieve global optimization while minimizing communication costs between different machines. The goal of the algorithm is to find a global optimum by iteratively updating the local models at each machine and exchanging information with the neighboring machines.\n",
        "\n",
        "Here is an algorithm based on distributed consensus optimization:\n",
        "\n",
        "1.   **Initialization**: Each machine $i$ initializes its own model parameters $x_i$.Each machine $i$ sends its model parameters to its neighboring machines.\n",
        "Each machine $i$ receives the model parameters of its neighbors.\n",
        "2.   **Consensus**: Each machine $i$ computes the average of its own model parameters and the model parameters received from its neighbors.\n",
        "Each machine $i$ updates its own model parameters to be the average value computed in the previous step.\n",
        "3.   **Gradient**: Each machine $i$ computes the gradient of its own local loss function with respect to its own model parameters.\n",
        "Each machine $i$ sends the gradient to its neighboring machines.\n",
        "Each machine $i$ receives the gradients from its neighbors.\n",
        "4.   **Update**: Each machine $i$ updates its own model parameters using the gradient and the step size.\n",
        "5.   **Repeat**: Repeat steps 2-4 until convergence.\n"
      ],
      "metadata": {
        "id": "DNZHw4uV3D-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the consensus function\n",
        "def consensus(x, neighbors):\n",
        "    sum = x.clone()\n",
        "    for neighbor in neighbors:\n",
        "        dist.send(tensor=x, dst=neighbor)\n",
        "        neighbor_x = torch.zeros_like(x)\n",
        "        dist.recv(tensor=neighbor_x, src=neighbor)\n",
        "        sum += neighbor_x\n",
        "    return sum / (len(neighbors) + 1)\n",
        "\n",
        "# Define the update function\n",
        "def update(x, gradient, step_size):\n",
        "    return x - step_size * gradient\n",
        "\n",
        "# Define the main function\n",
        "def run(rank, size):\n",
        "    # Initialize the model parameters\n",
        "    x = torch.tensor([0.0, 0.0], requires_grad=True)\n",
        "\n",
        "    # Define the neighbors\n",
        "    left_neighbor = (rank - 1) % size\n",
        "    right_neighbor = (rank + 1) % size\n",
        "    neighbors = [left_neighbor, right_neighbor]\n",
        "\n",
        "    # Run the consensus optimization algorithm\n",
        "    for i in range(10):\n",
        "        # Consensus step\n",
        "        x = consensus(x, neighbors)\n",
        "        print(x)\n",
        "        # Gradient step\n",
        "        grad = gradient(x,y,A_pytorch)\n",
        "        x_grad = torch.zeros_like(x)\n",
        "        dist.reduce(tensor=grad, dst=0)\n",
        "        dist.broadcast(tensor=x_grad, src=0)\n",
        "        x.grad = x_grad\n",
        "        x = update(x, x.grad, step_size=0.1)\n",
        "        print(x)\n",
        "    # Print the final model parameters\n",
        "    if rank == 0:\n",
        "        print(\"Final model parameters:\", x)\n",
        "\n",
        "# Define the initialization function\n",
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '172.28.0.12'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\n",
        "\n",
        "size = 4\n",
        "processes = []\n",
        "for rank in range(size):\n",
        "    p = mp.Process(target=init_process, args=(rank, size, run))\n",
        "    p.start()\n",
        "    processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "    p.join()"
      ],
      "metadata": {
        "id": "XIX3v2P63Cql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8/ (Bonus) By getting inspired by the code proposed in the section \"Distributed Training\" from the tutorial \"https://pytorch.org/tutorials/intermediate/dist_tuto.html\", implement a distributed version of your favorite ML algorithm. "
      ],
      "metadata": {
        "id": "KYowKMRBR0FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split, DistributedSampler"
      ],
      "metadata": {
        "id": "jyaIJFqLpf04"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device to use for computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lMqqQpn5wcbX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "class getDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "      data = data.astype(np.float32)\n",
        "      label = labels.astype(np.float32)\n",
        "\n",
        "      self.x_data = torch.from_numpy(data)\n",
        "      self.y_data = torch.from_numpy(labels)\n",
        "      self.len = data.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "      return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.len"
      ],
      "metadata": {
        "id": "HNdGMfhdddK8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LR model\n",
        "class LRModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LRModel, self).__init__()\n",
        "        self.fc =nn.Sequential(\n",
        "                nn.Linear(input_size, 3),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(3,1),\n",
        "                )\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "UMr5XBw-dqCL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "def train(rank, world_size):\n",
        "    # Set the seed to ensure reproducibility\n",
        "    \n",
        "    data = np.loadtxt(open('/content/sample_data/california_housing_train.csv', \"r\"), delimiter=\",\", skiprows=1)\n",
        "    train_data = getDataset(data[:,0:-1],data[:,-1])\n",
        "    torch.manual_seed(0)\n",
        "    # Set the batch size and number of epochs\n",
        "    batch_size = 64\n",
        "    num_epochs = 10\n",
        "    \n",
        "    # Create a distributed sampler to split the data across different processes\n",
        "    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank)\n",
        "    \n",
        "    # Create a data loader for the training data using the distributed sampler\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "    \n",
        "    # Create the LR model and move it to the device\n",
        "    input_size = data.shape[1]-1\n",
        "    model = LRModel(input_size).to(device)\n",
        "    \n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "    \n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "            data_m, target = data[0].to(device), data[1].to(device).float()\n",
        "            normalizer_train = len(train_loader.dataset)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data_m)\n",
        "            loss = criterion(output, target.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % 10 == 0:\n",
        "                print('Rank ', rank, ', Epoch ', epoch, ', Batch ', batch_idx, ', Loss: ', loss.item()/normalizer_train, '\\n')"
      ],
      "metadata": {
        "id": "nO7Cvn3hjKye"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_process(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29505'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\n",
        "size = 4\n",
        "processes = []\n",
        "\n",
        "for rank in range(size):\n",
        "  p = mp.Process(target=init_process, args=(rank, size, train))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "id": "shy_IZs3xA_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99760911-d599-4919-fe38-4f0a67d7e825"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank Rank Rank Rank   2 3 1  , Epoch 0, Epoch    0, Epoch , Epoch    , Batch 0 0  0 0, Batch   , Batch  , Batch , Loss:  0   0 0, Loss:  3015360.993882353, Loss:  , Loss:  3471007.744 3682982.731294118   3543142.1590588237\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Rank Rank  Rank 2  3, Epoch   , Epoch  0 0, Batch  , Batch    100 , Loss:  3609419.053176470610  Rank  \n",
            ", Epoch , Loss:  \n",
            "  14221015.2207058830   , Epoch \n",
            " , Batch \n",
            " 010  , Loss: Rank , Batch   3546466.1835294122 Rank 10   , Loss: \n",
            " 3, Epoch  \n",
            "  02694036.540235294 , Epoch   , Batch 0 \n",
            "Rank  20\n",
            "  , Batch 0 , Loss:  20 , Epoch  3037473.6112941178 , Loss:  0\n",
            " Rank 2633493.8051764704\n",
            "  1, Batch   \n",
            " 20, Epoch  \n",
            " , Loss: 0Rank   2548069.3157647057  2, Batch \n",
            "Rank   \n",
            " , Epoch 3, Epoch 20    00, Loss:   2980500.660705882 , Batch  \n",
            "\n",
            "Rank   , Batch 30 0Rank   , Epoch 30 1, Loss:  , Loss:   , Epoch   2934388.374588235300   3491232.3463529414 \n",
            ", Batch  , Batch \n",
            " \n",
            " 30\n",
            "30  , Loss: , Loss:   3010932.7362844894.147764706Rank  Rank   2\n",
            "\n",
            "  3\n",
            "\n",
            " , Epoch , Epoch Rank    Rank 00 0   , Batch , Epoch 1, Batch     40, Epoch 040   , Loss: 0 , Batch  , Loss:   40, Batch   2576875.52, Loss:  \n",
            "2617656.0188235296\n",
            "   40\n",
            "3660507.015529412 , Loss: \n",
            " \n",
            "\n",
            "Rank   3032744.4178823533 \n",
            " , Epoch Rank Rank  \n",
            "  210  , Epoch  , Epoch   0 , Batch 0 , Batch  , Batch   505050   , Loss: , Loss:  3052554.3604705883  3049767.1529411767, Loss:  \n",
            "Rank  \n",
            "\n",
            " 3013530.563764706\n",
            " \n",
            "0\n",
            " , Epoch  Rank Rank Rank 0   3 12  , Batch , Epoch   , Epoch   50, Epoch 00    , Loss: , Batch 0   2737549.7938823528, Batch 60  , Batch  \n",
            "60 , Loss: \n",
            "60   , Loss: 2302215.8305882355  , Loss: \n",
            " 2955574.332235294\n",
            " 2971127.326117647\n",
            "Rank  \n",
            "  0Rank \n",
            " 1, Epoch   Rank , Epoch 0   \n",
            "31, Batch    , Epoch , Batch 60   01 , Loss:  , Batch , Loss:    Rank 2710039.37129411752805760.2409411767 0  2\n",
            " \n",
            "\n",
            ", Loss: \n",
            "  2506573.703529412, Epoch   1 Rank \n",
            " , Batch Rank  0  0\n",
            ", Epoch  1 , Epoch 1   , Loss:  2707481.29882352941  \n",
            ", Batch \n",
            ", Batch   010 Rank , Loss:  Rank    , Loss: 22621119.30729411783   2259311.676235294\n",
            " \n",
            ", Epoch , Epoch  \n",
            "\n",
            " 1 1  , Batch  , Batch  1010 , Loss:  2641191.6348235295 Rank \n",
            " \n",
            " , Loss:  2187304.2371764705 0\n",
            "Rank   , Epoch  \n",
            "11  , Epoch , Batch   110 Rank  , Loss: , Batch  2Rank     20 2399448.36517647053, Epoch   , Loss:  1\n",
            ", Epoch   \n",
            " 3043111.393882353, Batch 1   \n",
            "20, Batch  \n",
            " , Loss: 20  Rank 2672218.8348235292, Loss:    0\n",
            "2005097.2912941177  , Epoch \n",
            "Rank \n",
            "  \n",
            "11  , Batch , Epoch   201  , Loss: , Batch   3434076.039529411630Rank   \n",
            " , Loss: Rank 2\n",
            "   32219780.6983529413, Epoch  1  \n",
            ", Epoch \n",
            " Rank , Batch   30  1, Loss:  2488600.93741176460   , Batch \n",
            ", Epoch  \n",
            " 30Rank   11 , Loss:   , Batch 2548436.2691764706, Epoch    30\n",
            "1Rank   , Epoch  2\n",
            " , Batch , Loss:  40  2795599.9924705881 , Loss:   \n",
            " \n",
            ", Batch Rank  3030496.9185882355 40 3\n",
            "  \n",
            ", Loss: , Epoch   2627539.18494117631Rank    , Batch \n",
            "0\n",
            " Rank  , Epoch   1140   , Epoch , Loss: , Batch    Rank 11610396.73223529440    2, Batch \n",
            ", Loss:   \n",
            " , Epoch 501948618.3905882353  1, Loss:   \n",
            " 2246921.035294118\n",
            "Rank , Batch    \n",
            "350\n",
            "  , Epoch , Loss:   12648972.5891764704Rank    , Batch \n",
            "0 \n",
            "Rank  50 , Epoch   1, Loss:  1 , Epoch 2295982.9232941177   , Batch \n",
            "\n",
            "1Rank   50 , Batch 2 60   , Loss: Rank  , Epoch  1  , Batch  60, Loss:  , Loss:  32687135.262117647 1286103.04 \n",
            "2126766.3209411763 \n",
            "  \n",
            "\n",
            "\n",
            "\n",
            ", Epoch Rank   1 2Rank , Batch  1  , Epoch  2  60, Epoch  , Batch  Rank 2 , Loss: , Batch  0 , Loss:    1830096.775529411900  \n",
            "\n",
            "  , Loss: 2133327.872, Epoch   2703463.6047058823 \n",
            "1  2 Rank  \n",
            "\n",
            ", Batch  , Epoch  60\n",
            " 2, Loss:  , Batch  1939891.8625882354Rank   Rank 101  3 , Loss:  , Epoch    \n",
            "1420581.10494117652 \n",
            " \n",
            "\n",
            ", Batch , Epoch  0  2, Loss: Rank    , Batch 02054648.771764706  10 , Epoch Rank  \n",
            " \n",
            ", Loss:  22 , Batch    , Epoch 2279015.845647059 0 2\n",
            " , Loss: Rank  \n",
            ", Batch    203 2290217.6828235295, Loss:   \n",
            "\n",
            "1926950.6710588236  \n",
            "Rank , Epoch  \n",
            " 1 , Epoch  2 Rank , Batch  10  2, Loss:   01907933.5454117646Rank , Batch    \n",
            " \n",
            ", Epoch  202  , Loss: 2, Batch   1647241.577411764810   , Epoch , Loss:  \n",
            "2 \n",
            ", Batch Rank  2868725.157647059  30 \n",
            " \n",
            "3, Loss:  Rank  1, Epoch   , Epoch  1727578.5938823532 2  , Batch  \n",
            " Rank \n",
            ", Batch 20  30 0  , Loss: , Loss:  , Epoch 1808604.4611764706  2085618.145882353  \n",
            "2\n",
            "\n",
            "\n",
            " Rank , Batch   20Rank 2  Rank 3  , Loss:   , Epoch  , Epoch 1788803.9152941178 \n",
            " 12 \n",
            "2  , Epoch , Batch   240, Batch   30, Loss:    1884802.9515294118, Batch  , Loss:   \n",
            "Rank 0 \n",
            "40 , Epoch   1845004.64941176472, Loss:  , Batch   \n",
            "\n",
            " 2173649.37788235330 \n",
            " Rank  \n",
            ", Loss: 2 , Epoch  2484615.770352941 Rank  \n",
            " 2 \n",
            "3, Batch   , Epoch  50 Rank 2 , Loss: 1 , Epoch , Batch   1818824.5835294118Rank     \n",
            "240\n",
            " 0  , Batch  , Loss:  , Epoch  2502117890.7708235295 , Batch   \n",
            "40\n",
            " Rank , Loss:    , Loss: 1511599.1642352942   2258736.308705882Rank  \n",
            ", Epoch 3\n",
            "  \n",
            "2  \n",
            ", Batch  , Epoch 60 Rank 2  1 , Loss:   Rank , Batch , Epoch   02450716.7924705883  2  \n",
            ", Batch 50, Epoch  \n",
            " 60 , Loss:  , Loss:  1924176.9562352942 \n",
            " 2206778.06682352952\n",
            " Rank   , Batch Rank \n",
            "\n",
            " 250  , Loss:  2056550.76141176471 , Epoch   3 , Batch   \n",
            ", Epoch \n",
            " 0 , Loss: 3Rank   Rank 1726453.3985882353 , Batch  30  \n",
            ", Epoch \n",
            "  0 , Epoch 2 , Batch     2187720.342588235 , Loss: 2Rank 1 60 \n",
            " \n",
            ", Epoch  , Batch , Loss:  3  2480244.615529412 60, Batch    \n",
            ", Loss: Rank  \n",
            "10 , Loss:  21788928.2409411764   , Epoch 2775372.257882353\n",
            " \n",
            " \n",
            " 3Rank \n",
            " , Batch 100   , Loss: , Epoch Rank   Rank  31733705.728 31  \n",
            ", Batch   \n",
            ", Epoch  0  3, Loss:   , Batch  , Epoch 1994630.6861176470  \n",
            " , Loss: 3\n",
            "  2080108.7849411764, Batch Rank   \n",
            " Rank 20 2\n",
            "0 , Epoch    , Epoch , Loss: 3  2288289.91247058853   , Batch \n",
            " , Batch \n",
            " 20 Rank 10, Loss:    1854434.0028235293 3\n",
            ", Loss:   \n",
            "1561729.3854117647, Epoch   Rank 3  \n",
            ", Batch 1\n",
            "  , Epoch 10  Rank , Loss: 3  2 Rank , Batch 1543066.5637647058 , Epoch    \n",
            " 30\n",
            "0 3 , Batch  30 , Epoch , Loss:   3 1306384.9863529413 , Loss:   \n",
            "\n",
            ", Batch  2197100.664470588320Rank   Rank , Loss: \n",
            "   31\n",
            " , Epoch  31587577.5548235294 , Batch    40 , Loss: , Epoch \n",
            " \n",
            " 3 , Batch 1680320.7529411765Rank   \n",
            "0Rank \n",
            "   20, Epoch 2   , Loss: 3, Epoch    1949740.3331764706, Batch Rank 3  \n",
            "30 \n",
            "  1, Batch  , Loss: , Epoch    401740925.6508235293 3 Rank \n",
            "  \n",
            ", Loss: 3, Batch   1760071.68 50 , Epoch  3Rank \n",
            "   0\n",
            ", Loss:  , Batch  , Epoch 2227551.292235294  3 30\n",
            " Rank  \n",
            ", Batch  2, Loss:    40, Epoch   1374900.1035294118 3, Loss:  \n",
            ", Batch  Rank \n",
            " 1 1681441.8522352942  , Epoch 50\n",
            "\n",
            "  3 , Batch , Loss:  2077070.5167058823 60 , Loss: Rank   1471172.7284705883 \n",
            " \n",
            "\n",
            "\n",
            "3 Rank , Epoch  Rank 0  13 , Epoch   Rank , Epoch , Batch     3402 4 , Loss:  , Epoch  , Batch   2573353.9237647057 , Batch 3 50  \n",
            " 0\n",
            ", Batch , Loss:    , Loss: 601475995.166117647  1413302.1515294118  , Loss: \n",
            "\n",
            "\n",
            " 2028268.604235294 \n",
            "\n",
            "\n",
            "Rank  3Rank   Rank 0  , Epoch 32Rank  , Epoch  , Epoch    3 14, Batch   , Batch , Epoch    60  , Loss: 450, Batch     1631002.1421176470, Loss: , Batch     \n",
            "1606351.691294117610, Loss:   \n",
            " \n",
            "1266366.8254117647, Loss: \n",
            "  \n",
            "1468022.1816470588 \n",
            "\n",
            "\n",
            "Rank  Rank 0 3 Rank  , Epoch   Rank , Epoch 24    3, Batch 1, Epoch     , Batch 0, Epoch  4 , Loss:    601666297.97647058824, Batch   \n",
            " \n",
            " , Loss: , Batch 10  2069528.09411764720  \n",
            ", Loss: \n",
            "  1534861.312 , Loss: Rank \n",
            "  \n",
            "1577432.24470588243  Rank \n",
            ", Epoch \n",
            "  40Rank  , Batch   , Epoch 1  4 0 , Batch , Epoch   Rank 10 , Loss:  , Loss:   4 21579089.1971764707 1551779.5990588234  , Batch  \n",
            ", Epoch  \n",
            "30\n",
            " \n",
            "4  , Batch , Loss:   1486419.6065882354Rank 20  \n",
            "3 \n",
            "Rank   , Loss: , Epoch  0 1878355.00423529424   , Epoch , Batch \n",
            " 4 \n",
            "Rank   , Batch 1  20, Epoch   , Loss: 104   1794960.6851764705, Loss:   , Batch \n",
            "2235541.8654117645\n",
            "  40Rank \n",
            " \n",
            " , Loss: 2  2226711.3712941175, Epoch   Rank \n",
            "4Rank  \n",
            "  , Batch 0 , Epoch  3 4 30, Epoch    , Batch , Loss: 4   1799634.1007058823, Batch Rank  2030   \n",
            " , Loss: 1\n",
            ", Loss:    1307318.15152941181338914.816  \n",
            "\n",
            "\n",
            "\n",
            ", Epoch  4 Rank Rank Rank  , Batch  3 0  50, Epoch 2   , Epoch  , Loss: 4  , Epoch  , Batch 1374037.17270588254   430\n",
            "  , Loss: \n",
            " , Batch  , Batch  1248164.321882353 40 \n",
            " 40\n",
            ", Loss: Rank   , Loss:  1 , Epoch 1733358.049882353  4  1289370.3228235294, Batch \n",
            "  \n",
            "60\n",
            "Rank \n",
            " 3  , Epoch , Loss: Rank    02163207.9510588236 , Epoch 4  4\n",
            "  , Batch \n",
            ", Batch  Rank 40 , Loss:    21944221.3345882352  , Epoch 50\n",
            "  , Loss: \n",
            " 1433526.7538823534Rank   , Batch 1  , Epoch  , Loss: Rank    1385059.0268235295  35 \n",
            ", Batch , Epoch \n",
            " \n",
            "500 Rank 4 \n",
            "  0, Batch    , Epoch , Loss: 50  1260993.4757647058 4, Loss:   Rank 2 \n",
            ", Batch  \n",
            "  , Epoch 60 1321857.024, Loss:    41229291.8814117648\n",
            " \n",
            ", Batch  \n",
            "\n",
            " Rank  60 Rank 1, Loss: Rank    31531284.2992941176  , Epoch 0 , Epoch    \n",
            "5, Epoch 4  \n",
            ", Batch 5 , Batch    0 , Loss:  10, Batch 1488819.6216470588 \n",
            " \n",
            ", Loss:   60Rank  1288250.1872941176  , Loss: 2\n",
            " Rank \n",
            " , Epoch   1809044.4197647060  5 , Epoch \n",
            " \n",
            "5, Batch   0, Batch   Rank Rank  , Loss: 10   13, Loss:  1607097.7656470588, Epoch     1748441.9312941176\n",
            " , Epoch 5\n",
            " \n",
            " \n",
            ", Batch 5  0, Batch   20, Loss:   Rank , Loss: 1795409.1971764707Rank     21450634.0592941176\n",
            "0  \n",
            " , Epoch \n",
            ", Epoch  \n",
            " 55 , Batch  , Batch   20Rank Rank  10  , Loss: 1  3 , Loss: 1297975.0550588234, Epoch  5   , Batch \n",
            " , Epoch \n",
            " 30 1152087.9435294117 \n",
            " , Loss: 5  \n",
            "1173768.5534117648, Batch Rank    \n",
            "100\n",
            "  Rank , Loss:   , Epoch 1411077.5416470592   5, Epoch \n",
            " \n",
            " , Batch Rank 5   301, Batch    , Loss: , Epoch 20   1783557.7825882353Rank , Loss: 5    \n",
            "3, Batch 1307101.3044705882\n",
            "   , Epoch 40\n",
            " 5 \n",
            " , Batch  20, Loss:  , Loss:   1366906.7595294118 \n",
            "1001423.329882353 Rank  \n",
            "\n",
            "\n",
            "0 Rank , Epoch   25  , Epoch , Batch   40Rank Rank  5  , Loss:  31   , Batch  30, Epoch , Epoch   5 1112330.6014117647 , Batch , Loss:   5 1898262.046117647 30, Batch \n",
            "  \n",
            " , Loss:  1755329.3552941177 \n",
            "\n",
            "\n",
            "50\n",
            " , Loss:  1315206.4451764706 \n",
            "Rank Rank \n",
            "  Rank 0 23 , Epoch    5, Epoch , Epoch    Rank 55 , Batch , Batch  40    501, Loss: , Batch     1191726.3811764705, Epoch , Loss: 40 \n",
            "   \n",
            "51039219.8927058823, Loss:   1096835.072  , Batch \n",
            "\n",
            "\n",
            " \n",
            "60 Rank  , Loss: 2  1309456.384 , Epoch  \n",
            "5\n",
            "Rank Rank    , Batch 30   50, Epoch , Epoch   Rank , Loss: 5  1407593.1708235294, Batch   15    50\n",
            ", Epoch , Batch  \n",
            ", Loss:   60 1123295.232 6 \n",
            " , Loss: Rank , Batch    1469590.9496470587\n",
            " 02\n",
            "\n",
            "  , Loss: Rank , Epoch   0 , Epoch 1493823.1265882354 \n",
            " \n",
            "5 Rank , Batch   3  Rank 60 , Epoch , Loss:   51372263.6047058823   6, Batch 1\n",
            "   \n",
            "60, Batch , Epoch    , Loss: 06   1385272.5007058824Rank , Loss: , Batch     \n",
            "1210512.0828235294210\n",
            "   , Epoch \n",
            ", Loss:  \n",
            " 61417689.3289411764Rank    , Batch \n",
            "3 \n",
            " 0, Epoch  , Loss:  Rank 6   1712970.2098823530, Batch    \n",
            ", Epoch \n",
            "0Rank    , Loss: 61   977024.7228235294, Batch , Epoch    \n",
            "106 \n",
            ", Loss:   Rank , Batch 1684484.698352941   2\n",
            "20 \n",
            " , Epoch , Loss:   Rank 61403729.5585882354   3, Batch \n",
            "  \n",
            "10, Epoch  Rank  6 , Loss:  0 , Batch  1263385.7807058825 10 , Epoch  \n",
            " Rank , Loss: \n",
            "6   1002364.3858823531, Batch    \n",
            ", Epoch 20\n",
            "  , Loss: Rank 6   1786270.17788235282 , Batch   \n",
            ", Epoch 30\n",
            " Rank  , Loss: 6   3826117.5416470588, Batch    \n",
            "20 , Epoch \n",
            "Rank , Loss:    601163615.4127058825   \n",
            ", Epoch , Batch \n",
            "  206  Rank , Loss: , Batch   30 1448767.84941176461 , Loss: \n",
            "Rank \n",
            "     6 , Epoch  1241938.5223529412\n",
            "   6, Batch \n",
            " , Epoch , Batch 40  , Loss: 30  Rank 956691.2150588236, Loss:    1439772.9129411764Rank  3 \n",
            "\n",
            "0 \n",
            "\n",
            " , Epoch  , Epoch  66 , Batch  Rank Rank , Batch    3040 2  1 , Loss: , Loss:   , Epoch 1332091.6028235294 , Epoch  963937.9425882353   66\n",
            "\n",
            "\n",
            "\n",
            "  , Batch , Batch  50  Rank  , Loss: 40  1356005.13505882350, Loss:  Rank    3\n",
            "1034011.2263529411\n",
            " , Epoch   \n",
            "6\n",
            " , Batch , Epoch   506  Rank , Loss: Rank , Batch     1465741.67341176471402    \n",
            ", Loss: , Epoch  , Epoch \n",
            " 1568877.2668235295 6 6\n",
            "  \n",
            ", Batch , Batch   5060 Rank  , Loss:   , Loss: 1773418.73694117650 \n",
            " \n",
            " 1735921.182117647 Rank , Epoch \n",
            "\n",
            "  3 6, Epoch   , Batch  606Rank   , Loss:   , Batch 1105601.3552941177Rank 1  50 \n",
            " 2 \n",
            " , Epoch  , Loss:  , Epoch  67523695.28470588237   \n",
            ", Batch \n",
            ", Batch Rank    0 , Epoch  600 , Loss:  7  , Loss:  , Batch 768060.7774117647 0697567.7741176471   , Loss: Rank \n",
            "\n",
            " \n",
            " \n",
            "3 , Epoch 1187129.344  6 \n",
            ", Batch Rank \n",
            " 602 Rank   , Epoch  1 , Loss:  7, Epoch   , Batch 7 Rank  1117754.428235294  0 , Batch \n",
            " 0 , Loss: \n",
            " 10 1302663.649882353 , Epoch  , Loss:   \n",
            "7\n",
            "1126283.6254117647Rank    , Batch \n",
            " 310  \n",
            ", Epoch , Loss:  7 , Batch   Rank 1802737.664 Rank 2  \n",
            "0\n",
            " , Loss:  , Epoch  1235521.41552941181  \n",
            "7 \n",
            ", Epoch   , Batch 7Rank   100   , Loss: , Batch Rank  , Epoch  910465.8070588235 20  3\n",
            " 7\n",
            "  , Epoch , Loss: , Batch   20 7 1233725.0785882352, Loss:    \n",
            "Rank 971355.6781176471, Batch \n",
            "  10\n",
            "  , Loss: 2 \n",
            " 830136.5609411765, Epoch Rank   1 \n",
            " , Epoch 7 \n",
            " Rank , Batch  0 7 , Epoch  20 Rank , Batch    3730   , Loss: , Batch , Epoch    , Loss: 7 1317288.2974117647, Loss: , Batch  1019361.2830   \n",
            " \n",
            "\n",
            "\n",
            " 1046486.919529411720 \n",
            "\n",
            "Rank   , Loss: 2 Rank  , Epoch   Rank 789962.6315294118  017  \n",
            ", Epoch  \n",
            " , Batch , Epoch 7  , Batch   40730  , Loss:  , Loss:  1303865.5849411765 , Batch 962261.2329411765  Rank  40\n",
            "  \n",
            "\n",
            "3\n",
            ", Loss:  Rank  , Epoch 939028.961882353  7\n",
            "  \n",
            "2, Batch   30Rank , Epoch  , Loss:    1280928.2258823531 7\n",
            "Rank  , Batch    40, Epoch \n",
            "0 , Epoch   77  , Batch  Rank  , Loss: , Batch   5050  3, Loss:  , Epoch 950217.7882352942   \n",
            "\n",
            ", Loss: 1092059.738352941  \n",
            "1072272.8056470589 \n",
            "7  \n",
            "Rank  \n",
            ", Batch 2 Rank  40 , Epoch Rank   1 , Loss: 7  0 931656.4028235294 , Batch , Epoch  \n",
            ", Epoch  \n",
            "  507 7 , Loss:   , Batch 641623.4616470588 60, Batch   \n",
            "\n",
            " , Loss:  Rank  60 1220192.37647058813, Loss:    , Epoch \n",
            "1412626.6729411765  \n",
            "7\n",
            "  Rank , Batch \n",
            "2 , Epoch   7 50, Batch  60 , Loss:  Rank  Rank , Loss:  1504396.4687058823  0 1920159.6536470589   \n",
            ", Epoch \n",
            "\n",
            "\n",
            ", Epoch   88 , Batch   Rank , Batch 0  Rank , Loss:  0 21217234.462117647 \n",
            "   \n",
            "3, Loss:   959308.3783529411, Epoch  \n",
            " , Epoch \n",
            "8Rank   7, Batch   1, Batch   , Epoch   8 060Rank    , Batch  10, Loss:  0, Loss:    , Loss: , Epoch  8 1184075.1736470591100706.7557647058, Batch   10   , Loss: 932656.8508235294\n",
            " \n",
            "1139215.4202352942  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Rank Rank  0 , Epoch Rank   3 2 8 , Epoch  Rank , Batch   , Epoch 20   8, Loss: 8 , Batch   1460159.909647059 110 , Batch \n",
            "  , Loss: \n",
            " , Epoch  8 0  1191172.577882353, Batch  , Loss: \n",
            " 1151459.9303529412\n",
            " 20 \n",
            " \n",
            "Rank , Loss:  Rank   021219485.5755294117   , Epoch Rank , Epoch \n",
            "  \n",
            " 838   , Batch , Epoch , Batch    Rank 20308    , Loss: , Loss: , Batch   1 865569.310117647  1338763.3844705883, Epoch \n",
            " \n",
            "10 \n",
            "\n",
            "8  , Batch , Loss:  Rank  825839.6762352941 30  \n",
            "2\n",
            " , Loss: , Epoch Rank   0868386.9967058824 Rank  8 \n",
            " , Epoch   3\n",
            "8, Batch   , Epoch  , Batch 30   , Loss: 408 984944.2183529412   , Batch , Loss: \n",
            "  \n",
            "201324557.0108235294Rank    , Loss: 1\n",
            "\n",
            " 1054196.4348235293  \n",
            "\n",
            ", Epoch  Rank 8Rank    2, Batch   , Epoch 400 Rank  , Loss: 8   703108.638117647 , Epoch , Batch 3   \n",
            " 408, Epoch \n",
            "  , Batch   , Loss: 850   , Batch , Loss: 825890.6955294118   30907549.8164705882 \n",
            " Rank \n",
            ", Loss:  1112716.5891764706 \n",
            " \n",
            "\n",
            "\n",
            "1 , Epoch  8Rank   , Batch 2Rank    50, Epoch 3  , Epoch , Loss:  Rank 8 , Batch   50  716544.9637647059 08 , Loss:   , Epoch \n",
            " , Batch  \n",
            " 1133562.69929411778 40\n",
            "  , Loss: , Batch \n",
            "  60839170.4094117647  \n",
            "Rank \n",
            ", Loss:   Rank 1284990.6145882353 1\n",
            " \n",
            ", Epoch  Rank 2 8   , Batch , Epoch 3 Rank 60    , Epoch 80, Loss:     8, Batch 1261344.4065882354, Epoch     60, Batch \n",
            " 9 \n",
            "50  , Loss: , Batch  , Loss:  1094572.1524705882 0 754574.5167058824 Rank \n",
            " , Loss:  \n",
            " \n",
            "1832998.0988235294\n",
            "  , Epoch \n",
            " \n",
            "Rank Rank 9  3 , Batch 2  Rank  , Epoch  0  , Epoch , Loss: 8  , Batch 0 60 9, Epoch     1051879.5444705882, Loss: , Batch  9   \n",
            ", Batch 0934065.2724705882   \n",
            "\n",
            "\n",
            ", Loss: 10 1305496.2748235294  \n",
            "\n",
            ", Loss:  Rank 1078324.5251764706 \n",
            "Rank  2Rank  , Epoch    1 \n",
            "3, Epoch  9  9, Epoch   , Batch Rank 9, Batch    100, Batch    10, Epoch  , Loss:  9   0, Loss: , Batch  1330867.0192941176   20, Loss: 1188079.616\n",
            "  \n",
            "1114710.979764706 , Loss:   \n",
            "\n",
            "1043278.0649411764\n",
            "\n",
            " \n",
            "\n",
            "Rank  1 , Epoch Rank   Rank 90 Rank  , Batch   , Epoch 32  9  20, Epoch , Epoch     , Loss: 99  800390.6258823529, Batch  , Batch , Batch    \n",
            " 20\n",
            "30 10  , Loss: , Loss: , Loss:  800487.1830588236  999809.7468235294 997392.8658823529  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Rank \n",
            " 1 Rank , Epoch Rank    092Rank     , Epoch , Batch , Epoch 3   9 930  , Batch  , Epoch , Batch  , Loss:  30 9 40  1100451.358117647 , Loss:  , Batch , Loss: \n",
            "  \n",
            "20  823735.5369411765963959.9284705883  , Loss: \n",
            "\n",
            " \n",
            "\n",
            "1643659.8663529411 \n",
            "\n",
            "Rank  1Rank Rank   , Epoch 2   9Rank  , Epoch 0, Batch    , Epoch 39    409, Batch , Epoch     , Loss: , Batch 40 9   1013157.888, Loss: 50, Batch  \n",
            "   , Loss: 301132221.6207058823  \n",
            " 683350.8592941177\n",
            ", Loss:  \n",
            "\n",
            " \n",
            "803741.5755294118 \n",
            "Rank \n",
            "Rank   Rank 2 01 , Epoch  , Epoch   , Epoch 9   99, Batch  Rank 50  , Loss:  , Batch , Batch    3 682433.355294117750   60\n",
            "\n",
            ", Loss:  , Epoch   , Loss: 1151543.296  \n",
            "1009335.6574117647\n",
            " \n",
            "\n",
            "9 , Batch  Rank Rank 40 2 , Epoch  9   1, Loss: , Batch    60, Epoch   573461.0823529412 9, Loss: \n",
            "  \n",
            ", Batch 700379.6178823529  60\n",
            " \n",
            ", Loss:  1155124.0432941176 Rank \n",
            " \n",
            "3 , Epoch  9 , Batch  50 , Loss:  606578.3265882353 \n",
            "\n",
            "Rank  3 , Epoch  9 , Batch  60 , Loss:  958020.5477647058 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}